{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Introduction\n",
    "\n",
    "In this notebook, we will perform the basic computation of a one-dimensional analysis: spectrum and light curve. We will reduce some H.E.S.S. observations of the Crab Nebula applying the 1D geometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import astropy.units as u\n",
    "from astropy.coordinates import SkyCoord, Angle\n",
    "from astropy.time import Time\n",
    "from regions import PointSkyRegion, CircleSkyRegion\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "import warnings\n",
    "\n",
    "from gammapy.maps import MapAxis, WcsGeom, RegionGeom\n",
    "from gammapy.data import DataStore, Observation\n",
    "from gammapy.datasets import SpectrumDataset, Datasets, FluxPointsDataset\n",
    "from gammapy.makers import (\n",
    "    SpectrumDatasetMaker,\n",
    "    WobbleRegionsFinder,\n",
    "    ReflectedRegionsBackgroundMaker,\n",
    "    SafeMaskMaker,\n",
    ")\n",
    "from gammapy.modeling.models import (\n",
    "    PowerLawSpectralModel,\n",
    "    ExpCutoffPowerLawSpectralModel,\n",
    "    SkyModel,\n",
    "    create_crab_spectral_model,\n",
    "    ConstantTemporalModel,\n",
    "    GaussianTemporalModel,\n",
    ")\n",
    "from gammapy.modeling import Fit\n",
    "from gammapy.estimators import FluxPointsEstimator, LightCurveEstimator\n",
    "from plot_utils import plot_gammapy_lc_points, plot_gammapy_sed\n",
    "from gammapy.visualization import plot_spectrum_datasets_off_regions\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Spectrum and Light Curve Computation of the Crab Nebula\n",
    "\n",
    "Let us load the observations of the Crab Nebula. You can check which observation (id) belongs to which source on the last three pages of [this](https://www.mpi-hd.mpg.de/HESS/pages/dl3-dr1/hess_dl3_dr1.pdf) document and add them in the `obs_id` list below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore = DataStore.from_dir(\"data/hess-dl3-dr1/\")\n",
    "obs_ids = [23523, 23526, 23559, 23592]\n",
    "observations = datastore.get_observations(obs_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Data reduction \n",
    "\n",
    "We do not need all the information in the Observation (i.e. in the DL3 file) for our analysis. As we aim to perform a one-dimensional analysis we need counts from the signal and background regions and a reduced version of the IRF components (computed at a specific offset in the camera offset). This data reduction is performed by the `SpectrumDatasetMaker`.\n",
    "\n",
    "### 1.1.1. Energy binning and on region\n",
    "First we have to define a signal extraction region, also known as on region, inside which we want to compute the spectrum. In the simplest case this is just a `CircleSkyRegion`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for the spectrum extraction\n",
    "# energy axes\n",
    "# estimated energy for the counts histograms\n",
    "energy_axis = MapAxis.from_energy_bounds(\n",
    "    10, 1e5, nbin=20, per_decade=False, unit=\"GeV\", name=\"energy\"\n",
    ")\n",
    "# true energy for folding (integrating) the IRF\n",
    "# advice: make this denser than the estimated energy axis\n",
    "energy_true_axis = MapAxis.from_energy_bounds(\n",
    "    10, 1e5, nbin=28, per_decade=False, unit=\"GeV\", name=\"energy_true\"\n",
    ")\n",
    "\n",
    "# on region geometry\n",
    "target_position = SkyCoord(ra=83.63, dec=22.01, unit=\"deg\", frame=\"icrs\")\n",
    "#or SkyCoord.from_name(\"Crab Nebula\")\n",
    "\n",
    "on_region_radius = Angle(\"0.2 deg\")\n",
    "on_region = CircleSkyRegion(center=target_position, radius=on_region_radius)\n",
    "on_region_geom = RegionGeom.create(region=on_region, axes=[energy_axis])\n",
    "# number of OFF regions\n",
    "n_off_regions = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2. Exclusion mask\n",
    "\n",
    "We will use the reflected regions method to place off regions to estimate the background level in the on region defined above. To make sure the off regions don’t contain gamma-ray emission, we create an exclusion mask. There is only one known gamma-ray source near the Crab nebula: the AGN [RGB J0521+212](http://gamma-sky.net/#/cat/tev/23) at GLON = 183.604 deg and GLAT = -8.708 deg. We create the exclusion mask around that point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclusion_region = CircleSkyRegion(\n",
    "    center=SkyCoord(183.604, -8.708, unit=\"deg\", frame=\"galactic\"),\n",
    "    radius=0.5 * u.deg,\n",
    ")\n",
    "\n",
    "skydir = target_position.galactic\n",
    "geom = WcsGeom.create(\n",
    "    npix=(150, 150), binsz=0.05, skydir=skydir, proj=\"TAN\", frame=\"icrs\"\n",
    ")\n",
    "\n",
    "exclusion_mask = ~geom.region_mask([exclusion_region])\n",
    "exclusion_mask.plot()\n",
    "plt.plot(target_position.to_pixel(geom.wcs)[0],\n",
    "         target_position.to_pixel(geom.wcs)[1],\n",
    "         \"X\",label=\"Crab Nebula\")\n",
    "on_region.to_pixel(geom.wcs).plot(color=\"red\", label=\"on region\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3. Running the data reduction\n",
    "\n",
    "We create an empty `SpectrumDataset` and a `Maker` to fill in the off counts and on counts of each dataset. After that we have a collection of datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the blueprint for the dataset\n",
    "dataset_empty = SpectrumDataset.create(\n",
    "    geom=on_region_geom, energy_axis_true=energy_true_axis\n",
    ")\n",
    "# defines what is to be computed (\"made\")\n",
    "dataset_maker = SpectrumDatasetMaker(\n",
    "    containment_correction=False, selection=[\"counts\", \"exposure\", \"edisp\"]\n",
    ")\n",
    "\n",
    "# finds the position of the OFF regions\n",
    "region_finder = WobbleRegionsFinder(n_off_regions=n_off_regions)\n",
    "# \"makes\" the OFF counts\n",
    "bkg_maker = ReflectedRegionsBackgroundMaker(region_finder=region_finder, exclusion_mask=exclusion_mask)\n",
    "safe_mask_maker = SafeMaskMaker(methods=[\"aeff-max\"], aeff_percent=10)\n",
    "\n",
    "datasets = Datasets()\n",
    "\n",
    "for observation in observations:\n",
    "    # fills the ON counts and evaluates the IRF at the offset of interest\n",
    "    dataset = dataset_maker.run(\n",
    "        dataset_empty.copy(name=str(observation.obs_id)), observation\n",
    "    )\n",
    "    # fills the OFF counts\n",
    "    dataset_on_off = bkg_maker.run(dataset, observation)\n",
    "    #dataset_on_off = safe_mask_maker.run(dataset_on_off, observation)\n",
    "    datasets.append(dataset_on_off)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4. Check the dataset\n",
    "\n",
    "We can take a moment to examine our dataset and set up, for example, the correct energy threshold for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to speed-up the fit - just for the average spectrum estimation - let us use the stacked dataset\n",
    "dataset_stack = datasets.stack_reduce()\n",
    "dataset_stack.peek()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply proper minimum and maximum energy for the fit\n",
    "e_min = 600 * u.GeV\n",
    "e_max = 20 * u.TeV\n",
    "dataset_stack.mask_fit = dataset_stack.counts.geom.energy_mask(e_min, e_max)\n",
    "\n",
    "dataset_stack.peek()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4. Plot the OFF regions\n",
    "\n",
    "To visualize which off regions the ReflectedRegionsMaker has chosen and check wether there are any overlaps with the exclusion region or on region we can plot all of them on a map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "ax = exclusion_mask.plot()\n",
    "on_region.to_pixel(ax.wcs).plot(ax=ax, edgecolor=\"k\")\n",
    "plot_spectrum_datasets_off_regions(ax=ax, datasets=datasets)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Source statistics\n",
    "\n",
    "We can look at the overall source statistics in our signal extraction region (on region) and make the corresponding plots. For every observation we find the on counts, off counts and excess counts (on - off) and significance value of the signal. The increase of excess counts with time and the increase of significance of the signal with time should follow a linear function and a square-root function, respectively. In this case we cannot say much about it since the number of observations we have is very limited and the data can fluctuate around these functions but never follows them perfectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_table = datasets.info_table(cumulative=True)\n",
    "display(info_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax_excess, ax_sqrt_ts) = plt.subplots(figsize=(10, 4), ncols=2, nrows=1)\n",
    "ax_excess.plot(\n",
    "    info_table[\"livetime\"].to(\"h\"),\n",
    "    info_table[\"excess\"],\n",
    "    marker=\"o\",\n",
    "    ls=\"none\",\n",
    ")\n",
    "\n",
    "ax_excess.set_title(\"Excess\")\n",
    "ax_excess.set_xlabel(\"Livetime [h]\")\n",
    "ax_excess.set_ylabel(\"Excess events\")\n",
    "\n",
    "ax_sqrt_ts.plot(\n",
    "    info_table[\"livetime\"].to(\"h\"),\n",
    "    info_table[\"sqrt_ts\"],\n",
    "    marker=\"o\",\n",
    "    ls=\"none\",\n",
    ")\n",
    "\n",
    "ax_sqrt_ts.set_title(\"Sqrt(TS)\")\n",
    "ax_sqrt_ts.set_xlabel(\"Livetime [h]\")\n",
    "ax_sqrt_ts.set_ylabel(\"Sqrt(TS)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Spectrum fit and flux points estimation of the Crab Nebula\n",
    "\n",
    "To estimate the LC in Gammapy a spectral model is needed. \n",
    "\n",
    "### 1.3.1. Spectrum computation\n",
    "\n",
    "We will use the one we obtain by fitting the stacked dataset. It is however also possible to do a joint fit of the datasets, meaning that we fit each of them individually but simultaneously. You can try this in the excercises, using also MAGIC data to do a joint fit. The Crab Nebula is a very well characterized source in VHE astronomy, therefore we know which spectral shape its emission follows: a power law with an exponential cutoff. The corresponding spectral model we have to choose in gammapy is the `ExpCutoffPowerLawSpectralModel`. Let's have a look at the mathematical definition: [ExpCutoffPowerLawSpectralModel](https://docs.gammapy.org/dev/user-guide/model-gallery/spectral/plot_exp_cutoff_powerlaw.html)\n",
    "\n",
    "Similarly, for some other AGN sources it was possible to detect a spectral cut off, but note that the easiest and most common spectral shape in VHE astronomy is the simple power law. We use it for many analysis, in gammapy it is called `PowerLawSpectralModel`. You can find a full list of gammapy spectral models under the [Model Gallery](https://docs.gammapy.org/dev/user-guide/model-gallery/index.html), along with other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us fit the whole dataset to obtain an average specturm that we will then use for the LC computation\n",
    "spectral_model = ExpCutoffPowerLawSpectralModel(\n",
    "    amplitude=1e-12 * u.Unit(\"cm-2 s-1 TeV-1\"),\n",
    "    index=2,\n",
    "    lambda_=0.1 * u.Unit(\"TeV-1\"),\n",
    "    reference=1 * u.TeV,\n",
    ")\n",
    "# advice: if a LC with ULs has to be computed with this model, enlarge the amplitude range\n",
    "spectral_model.parameters[\"amplitude\"].min = 1e-15\n",
    "spectral_model.parameters[\"amplitude\"].max = 1e-7\n",
    "\n",
    "model = SkyModel(spectral_model=spectral_model, name=\"crab\")\n",
    "\n",
    "# assign the model to the dataset, don't forget this!\n",
    "dataset_stack.models = [model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = Fit()\n",
    "results = fit.run(datasets=dataset_stack)\n",
    "print(results)\n",
    "print(spectral_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us also compute flux points, to have a cross-check of the overall spectral shape\n",
    "# I take the edges of the flux points from the estimated energy axis\n",
    "energy_edges = energy_axis.edges[4:-3]\n",
    "\n",
    "flux_points_estimator = FluxPointsEstimator(\n",
    "    energy_edges=energy_edges,\n",
    "    source=\"crab\",\n",
    "    selection_optional=\"all\",\n",
    "    n_sigma_ul=2,\n",
    ")\n",
    "\n",
    "flux_points = flux_points_estimator.run(datasets=dataset_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a plot of the spectrum\n",
    "fig, ax = plt.subplots()\n",
    "plot_kwargs = {\n",
    "    \"energy_bounds\": [e_min, e_max],\n",
    "    \"sed_type\": \"e2dnde\",\n",
    "    \"yunits\": u.Unit(\"TeV cm-2 s-1\"),\n",
    "    \"xunits\": u.GeV,\n",
    "}\n",
    "\n",
    "# plot spectral model\n",
    "spectral_model.plot(\n",
    "    ax=ax,\n",
    "    ls=\"--\",\n",
    "    lw=2,\n",
    "    color= \"k\",\n",
    "    **plot_kwargs,\n",
    ")\n",
    "spectral_model.plot_error(ax=ax, facecolor= \"k\", alpha=0.4, **plot_kwargs)\n",
    "\n",
    "flux_points.plot(\n",
    "    ax=ax,\n",
    "    ls=\"\",\n",
    "    markeredgewidth=0,\n",
    "    color= \"k\",\n",
    "    label=\"crab\",\n",
    "    sed_type=plot_kwargs[\"sed_type\"],\n",
    ")\n",
    "\n",
    "create_crab_spectral_model(\"hess_ecpl\").plot(\n",
    "    **plot_kwargs,\n",
    "    label=\"crab hess\",\n",
    ")\n",
    "\n",
    "create_crab_spectral_model(\"magic_ecpl\").plot(\n",
    "    **plot_kwargs,\n",
    "    label=\"crab magic\",\n",
    ")\n",
    "ax.set_xlabel(r\"$E\\,/\\,{\\rm GeV}$\")\n",
    "ax.set_ylabel(\n",
    "    r\"$E^2 {\\rm d}\\phi/{\\rm d}E\\,/\\,({\\rm TeV}\\,{\\rm cm}^{-2}\\,{\\rm s}^{-1})$\"\n",
    ")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2. A note on spectrum and flux point estimation\n",
    "\n",
    "**Forward folding**\n",
    "The spectrum fit and the flux points are done via a so-called forward folding. Remember that we learned about the conversion from the \"detector world\" to the \"physical world\" using the IRFs? Forward folding refers to the conversion from true to estimated energy, using the energy dispersion matrix. The inverse computation is called unfolding and would require to invert the matrix to convert from estimated to true energy. The second is what we really want to do, but it is a complex task, since the migration matrix is not invertable in most cases.\n",
    "\n",
    "**What is the use of forward folding?**\n",
    "In simple words, the spectrum fit is a broadband fit to a large interval of energies (with amplitude and spectral index as free parameters), while the flux points are obtained refitting the best-fit broadband spectral shape to each bin separately (this time only the amplitude as free parameter). The result of the broadband fit is a best-fit spectral model parametrized in true energy. This is the dashed line in our plot above. The flux points and their binning, however, are defined in estimated energy. Considering this and the fact that out plot above is showing true energy on the x-axis, we can say that it is not very correct to plot it like this, since the bin edges for the flux points are actually in estimated energy. We could put it like this: Computing flux points with the forward folding technique is a sanity check for our overall spectral shape. Let's consider a case in which the emission of a source has a cutoff at high energies, but we assumed a power law shape. We can fit this to our data and not realize that there is a slight inconsistency with the data at the highest energies because the overall fitted spectrum can only tell us rough features, not the fine details. But when we compute flux points we will notice that the ones at highest energies are not fluctuating around our fit but systematically lower, indicating a cutoff of the spectrum. This is what forward-folding flux points are useful for. To have a cross check of the overall shape of the spectrum, even if we should not take them too seriously in general.\n",
    "\n",
    "**Technically how does it work?**\n",
    "In forward folding we assume a spectral shape of the source emission in the physical world (so parametrized in true energy). This spectral shape is parametrized in a model with a certain number of free parameters. To find the optimal values for the model we do the following: We start with an educated guess of the parameters and then iterate these steps...\n",
    "\n",
    " 1. We pass it through the IRFs (the energy dispersion matrix) and predict a count value in each bin of estimated energy (see plot below).\n",
    " 2. For each of these bins we have a poissonian probability distribution for the counts. We compute the likelihood as a product of all of these. The likelihood function tells us how probable the model with the corresponding parameters is, given our data. We aim to maximize it. \n",
    "\n",
    "...until we have found the maximum of the likelihood function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_spectrum, ax_residuals = dataset_stack.plot_fit()\n",
    "#ax_spectrum.set_ylim(0.1, 40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 Compute the light curve\n",
    "\n",
    "To investigate the variability of the source, we will compute a light curve with run-wise binning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the best-fit model to the joint datassets\n",
    "datasets.models = [model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let's compute a run-wise LC\n",
    "lc_maker_run_wise = LightCurveEstimator(\n",
    "    energy_edges=[e_min, e_max],\n",
    "    source=\"crab\",\n",
    "    time_intervals=None,\n",
    "    reoptimize=False,\n",
    "    n_sigma_ul=2,\n",
    "    selection_optional=\"all\",\n",
    ")\n",
    "lc_run_wise = lc_maker_run_wise.run(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(lc_run_wise.to_table(sed_type=\"flux\", format=\"lightcurve\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(\n",
    "    figsize=(7, 5),\n",
    "    gridspec_kw={\"left\": 0.16, \"bottom\": 0.2, \"top\": 0.98, \"right\": 0.98},\n",
    ")\n",
    "lc_run_wise.plot(ax=ax, marker=\"o\", label=\"Crab Nebula\")\n",
    "ax.set_ylim(8e-13, 1.5e-12)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.4 Night-wise light curve\n",
    "\n",
    "The light curve can be rebinned based on the objective of the analyzer, here we do the only other binning that makes sense for such a small dataset, a night-wise binning. We define the time intervals that cover the three nights of our observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_intervals = [\n",
    "    Time([\"2004-12-04T00:00\", \"2004-12-05T00:00\"], scale=\"utc\"),\n",
    "    Time([\"2004-12-06T00:00\", \"2004-12-07T00:00\"], scale=\"utc\"),\n",
    "    Time([\"2004-12-08T00:00\", \"2004-12-09T00:00\"], scale=\"utc\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_maker_nightwise = LightCurveEstimator(\n",
    "    energy_edges=[e_min, e_max],\n",
    "    time_intervals=time_intervals,\n",
    "    source=\"crab\",\n",
    "    reoptimize=False,\n",
    "    n_sigma_ul=2,\n",
    "    selection_optional=\"all\",\n",
    ")\n",
    "\n",
    "nightwise_lc = lc_maker_nightwise.run(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(\n",
    "    figsize=(7, 5),\n",
    "    gridspec_kw={\"left\": 0.16, \"bottom\": 0.2, \"top\": 0.98, \"right\": 0.98},\n",
    ")\n",
    "nightwise_lc.plot(ax=ax, marker=\"o\", color=\"tab:orange\", label=\"Crab Nebula\")\n",
    "ax.set_ylim(8e-13, 1.5e-12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.5. Fit the light curve\n",
    "\n",
    "We can even fit the lightcurve with a temporal model for the emission. here we try the easiest one, a constant temporal model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the flux point datasets by iterating over the returned lightcurve\n",
    "datasets_fp = Datasets()\n",
    "\n",
    "for idx, fp in enumerate(lc_run_wise.iter_by_axis(axis_name=\"time\")):\n",
    "    dataset_fp = FluxPointsDataset(data=fp, name=f\"time-bin-{idx}\")\n",
    "    datasets_fp.append(dataset_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model:\n",
    "temporal_model1 = ConstantTemporalModel()\n",
    "\n",
    "model = SkyModel(\n",
    "    spectral_model=spectral_model,\n",
    "    temporal_model=temporal_model1,\n",
    "    name=\"model-constant\",\n",
    ")\n",
    "\n",
    "datasets_fp.models = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = Fit()\n",
    "result = fit.run(datasets=datasets_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(\n",
    "    figsize=(7, 5),\n",
    "    gridspec_kw={\"left\": 0.16, \"bottom\": 0.2, \"top\": 0.98, \"right\": 0.98},\n",
    ")\n",
    "lc_run_wise.plot(ax=ax, sed_type=\"norm\", axis_name=\"time\")\n",
    "\n",
    "time_range = lc_run_wise.geom.axes[\"time\"].time_bounds\n",
    "temporal_model1.plot(ax=ax, time_range=time_range, label=\"Best fit model\")\n",
    "\n",
    "#ax.set_yscale(\"linear\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Excercises for this tutorial\n",
    "\n",
    "## 3.1.\n",
    "\n",
    "Load observations of the blazar PKS 2155−304 from the hess_dr1_dl3 data release, taken during a period in which the AGN was undergoing a **flaring episode** (check [here](https://www.mpi-hd.mpg.de/HESS/pages/dl3-dr1/hess_dl3_dr1.pdf)). Perform a 1D data reduction (taking into account possible differences like the exclusion masks and the coordinates of the source) and compute a spectral fit and flux points. Use a `PowerLawSpectralModel` (you can look up which parameters it gets on the gammapy documentation).\n",
    "\n",
    "## 3.2.\n",
    "\n",
    "Now compute a lightcurve of the emission of PKS 2155−304 in **flaring state**, using the best-fit spectral model obtained in the exercise above. Try to fit a `ConstantTemporalModel` to the lightcurve and check the goodness of the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kernel_gammapy-1.3",
   "language": "python",
   "name": "kernel_gammapy-1.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
